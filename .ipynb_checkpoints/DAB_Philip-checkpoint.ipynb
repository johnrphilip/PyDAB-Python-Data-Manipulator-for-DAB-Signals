{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0ca8df-adcd-447a-9120-567672b8acb7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### Setup:\n",
    "- [Standard Library Imports](#Standard-Library-Imports)\n",
    "- [Global Variables](#Global-Variables)\n",
    "\n",
    "### Data Operations:\n",
    "- [Load Data Function](#Load-Data-Function)\n",
    "- [Action 1: Clean Data Function](#Action-1-Clean-Data-Function)\n",
    "- [Save and Convert Function](#Save-and-Convert-Function)\n",
    "- [Extract DAB Function](#Extract-DAB-Function)\n",
    "- [Action 2: Join JSON 'Dictionary'](#Action-2-Join-JSON-Dictionary)\n",
    "- [Action 3: Mean, median, mode](#Action-3-Filter-and-Mean-Median-Mode)\n",
    "- [Action 4: Correlation: Chi-square Test Function](#Action-4-Correlation-Chi-square-Test-Function)\n",
    "- [Action 5: Visualize Data](#Action-5-Visualize-Data)\n",
    "\n",
    "### GUI Operations:\n",
    "- [Button Functions](#Button-Functions)\n",
    "- [Closing the App Functions](#Closing-the-App-Functions)\n",
    "- [GUI and Roots](#GUI-and-Roots)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c41e4b-b7b9-482d-a350-f46404ae5a5d",
   "metadata": {},
   "source": [
    "<a id='Standard-Library-Imports'></a>\n",
    "## Standard Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb09a55-c1ff-4b90-9397-700f6b5492ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Below is a list of the libraries needed to run this notebook. While I believe most of them are part of the standard Anaconda distribution,\n",
    "# Some of them may need to be installed. Particularly seaborn and chardet\n",
    "# !conda install chardet seaborn\n",
    "#pip3 install chardet seaborn\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "# additional imports\n",
    "import chardet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.figure import Figure\n",
    "from scipy.stats import chi2_contingency\n",
    "from tkinter import filedialog, simpledialog, messagebox, ttk\n",
    "import tkinter as tk\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6e278-02ee-4939-9221-43e41aaad5b1",
   "metadata": {},
   "source": [
    "<a id=\"global-variables\"></a>\n",
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc2a904-c6c8-4d0a-a4e4-7054c16b2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables:\n",
    "# `dataframes` is a list used to store the dataframes used throughout the application.\n",
    "# The others are flags to check if the data has been successfully loaded, cleaned and extracted.\n",
    "# They are also used for correct order of operation, user feedback and error management\n",
    "\n",
    "dataframes = []\n",
    "is_data_loaded = False\n",
    "is_data_cleaned = False\n",
    "is_data_extracted = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a70db-afe3-45b2-93ab-02f4c54bc4d8",
   "metadata": {},
   "source": [
    "<a id=\"Action-1-Clean-Data-Function\"></a>\n",
    "### Load Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c8e7c4-b020-4a7a-ad48-1018701a30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data function\n",
    "# This function loads 1 or more csv or json files, checks them for encoding and then joins them on the 'id' column.\n",
    "# There is an error check if no 'id' column exists\n",
    "# This also displays a small preview of the data so that the user has some idea of what has been loaded\n",
    "\n",
    "def load_data():\n",
    "    global is_data_loaded\n",
    "    temp_dfs = []  # local list for storing loaded dataframes\n",
    "    # allows loading the initial csv or the converted json\n",
    "    filenames = filedialog.askopenfilenames(title=\"Select file(s)\",\n",
    "                                            filetypes=((\"csv files\", \"*.csv\"), (\"json files\", \"*.json\")))\n",
    "\n",
    "    for filename in filenames:\n",
    "        print(f\"Loading file: {filename}...\")\n",
    "        text_box.insert(tk.END, f\"Loading file: {filename}...\\n\")\n",
    "\n",
    "        # Detect encoding with Chardet\n",
    "        with open(filename, 'rb') as f:\n",
    "            result = chardet.detect(f.read())\n",
    "        encoding = result['encoding']\n",
    "\n",
    "        # Load the data with correct encoding\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(filename, encoding=encoding)\n",
    "        elif filename.endswith('.json'):\n",
    "            df = pd.read_json(filename, lines=True, encoding=encoding)\n",
    "        else:\n",
    "            print(\"Invalid file type. Only CSV and JSON files are supported.\")\n",
    "            text_box.insert(tk.END, \"Invalid file type. Only CSV and JSON files are supported.\\n\")\n",
    "            return\n",
    "\n",
    "        print(f\"File loaded successfully with encoding {encoding}\")\n",
    "        text_box.insert(tk.END, f\"File loaded successfully with encoding {encoding}\\n\")\n",
    "        temp_dfs.append(df)\n",
    "\n",
    "    # Check if 'id' column exists in all dataframes\n",
    "    for temp_df in temp_dfs:\n",
    "        if 'id' not in temp_df.columns and 'ID' not in temp_df.columns:\n",
    "            print(f\"No 'id' or 'ID' column in one of the dataframes. Please check your data.\")\n",
    "            text_box.insert(tk.END, \"No 'id' or 'ID' column in one of the dataframes. Please check your data.\\n\")\n",
    "            return\n",
    "\n",
    "    # Convert all 'id' columns to lowercase for merging\n",
    "    for temp_df in temp_dfs:\n",
    "        if 'ID' in temp_df.columns:\n",
    "            temp_df.rename(columns={'ID': 'id'}, inplace=True)\n",
    "\n",
    "    # Merge dataframes on 'id' column\n",
    "    global dataframes  # modifying global variable\n",
    "    dataframes.clear()  # error check to clear existing dataframes if any\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='id'), temp_dfs)\n",
    "    dataframes.append(merged_df)\n",
    "\n",
    "    print(\"Files joined successfully on 'id' column\")\n",
    "    text_box.insert(tk.END, \"Files joined successfully on 'id' column\\n\")\n",
    "\n",
    "    # Display a small 5x5 preview of the joined dataframe for user feedback\n",
    "    preview = merged_df.iloc[:5, :5].to_string()  # example of slicing\n",
    "    print(\"Preview of the joined dataframe:\\n\", preview)\n",
    "    text_box.insert(tk.END, f\"Preview of the joined dataframe:\\n{preview}\\n\")\n",
    "    is_data_loaded = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3f05c-71db-45f4-afc8-012f1adbc3fd",
   "metadata": {},
   "source": [
    "<a id=\"clean-data-function\"></a>\n",
    "### Clean Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfea782b-4aa8-4116-adc5-b1eaff75ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action (1) clean_data function\n",
    "# This function not only performs action 1 for removing specific NGRs,\n",
    "# It also cleans the data and makes different preparations for the other steps such as whitespace,\n",
    "# converting datetime and standardizing column names \n",
    "\n",
    "def clean_data():\n",
    "    global is_data_loaded\n",
    "    global is_data_cleaned\n",
    "    if not is_data_loaded:\n",
    "        text_box.insert(tk.END, \"Please load the data before cleaning.\\n\")\n",
    "        return\n",
    "\n",
    "    if len(dataframes) == 1:\n",
    "        df = dataframes[0]  # Get the merged dataframe\n",
    "        df = df.copy()  # Make a copy of the dataframe for cleaning\n",
    "        text_box.insert(tk.END, \"Cleaning dataframe...\\n\")\n",
    "\n",
    "        # remove trailing whitespaces from column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        text_box.insert(tk.END, \"Removed trailing whitespaces from column names.\\n\")\n",
    "\n",
    "        # Remove trailing and leading whitespaces the rest of the dataframe\n",
    "        df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "        text_box.insert(tk.END, \"Removed trailing and leading whitespaces from all columns.\\n\")\n",
    "\n",
    "        # convert 'Date' column for use in the calculations for 3rd requirement\n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "            text_box.insert(tk.END, \"Converted 'Date' column to datetime.\\n\")\n",
    "\n",
    "        # Remove commas from 'In-Use ERP Total' column and convert to float\n",
    "        if 'In-Use ERP Total' in df.columns:\n",
    "            if df['In-Use ERP Total'].dtype == 'object':  # Check if column type is 'object' which is equivalent to string\n",
    "                df['In-Use ERP Total'] = df['In-Use ERP Total'].str.replace(',', '').astype(float)\n",
    "                text_box.insert(tk.END, \"Removed commas from 'In-Use ERP Total' column and converted it to float.\\n\")\n",
    "\n",
    "        # Filter rows where 'Date' is before 1950* this was an issue mentioned in the report, there are 7 entries for '1900' \n",
    "        # DAB radio was only invented in the 1990s\n",
    "        if 'Date' in df.columns:\n",
    "            before_1950 = df[df['Date'].dt.year < 1990]\n",
    "            df = df[df['Date'].dt.year >= 1950]\n",
    "            text_box.insert(tk.END, \"Filtered rows where 'Date' is before 1950.\\n\")\n",
    "\n",
    "        # Use numpy to replace all missing values \n",
    "        df.fillna(np.nan, inplace=True)\n",
    "        text_box.insert(tk.END, \"Replaced all missing values with numpy NaN.\\n\")\n",
    "\n",
    "        # Action 1 : Remove rows with specific NGR stations\n",
    "        if 'NGR' in df.columns:\n",
    "            df = df[~df['NGR'].isin(['NZ02553847', 'SE213515', 'NT05399374', 'NT252675908'])]\n",
    "            text_box.insert(tk.END, \"Removed rows with specific NGR stations.\\n\")\n",
    "\n",
    "        # Action 2 prep for column names for Action 2\n",
    "        # Rename columns\n",
    "        if 'In-Use ERP Total' in df.columns:\n",
    "            df.rename(columns={'In-Use ERP Total': 'Power(kW)'}, inplace=True)\n",
    "            text_box.insert(tk.END, \"Renamed 'In-Use ERP Total' to 'Power(kW)'.\\n\")\n",
    "\n",
    "        if 'In-Use Ae Ht' in df.columns:\n",
    "            df.rename(columns={'In-Use Ae Ht': 'Aerial height(m)'}, inplace=True)\n",
    "            text_box.insert(tk.END, \"Renamed 'In-Use Ae Ht' to 'Aerial height(m)'.\\n\")\n",
    "\n",
    "        if 'Freq.' in df.columns:\n",
    "            df.rename(columns={'Freq.': 'Freq'}, inplace=True)\n",
    "            text_box.insert(tk.END, \"Renamed 'Freq.' to 'Freq'.\\n\")\n",
    "\n",
    "        # standardize all column titles - could have went lowercase but seemed more appropriate for a column headings\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "        text_box.insert(tk.END, \"Standardized all column titles to uppercase.\\n\")\n",
    "\n",
    "        # Replace the old dataframe in the list with the cleaned dataframe as the new global dataframe to be used by other functions\n",
    "        dataframes[0] = df\n",
    "    \n",
    "        is_data_cleaned = True\n",
    "        text_box.insert(tk.END, \"Dataframe cleaned.\\n\")\n",
    "    else:\n",
    "        text_box.insert(tk.END, \"Please load the data before cleaning.\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7fe1d-204f-451f-804c-4412bd6714b4",
   "metadata": {},
   "source": [
    "<a id=\"Save-and-Convert-Function\"></a>\n",
    "### Save and Convert Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "906da2a3-64fa-49b3-a86a-f14d7a5f7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and convert JSON function. This meets functionality requirement to convert the dataset, \n",
    "# and additionally back up the dataset. The function on_closing() also helps fulfill this requirement\n",
    "\n",
    "def save_file():\n",
    "    if len(dataframes) == 0:\n",
    "        text_box.insert(tk.END, \"No data to save. Please load and clean data first.\\n\")\n",
    "        return\n",
    "\n",
    "    df = dataframes[0]  # Get the merged dataframe\n",
    "\n",
    "    try:\n",
    "        # Convert DataFrame to JSON string\n",
    "        json_str = df.to_json(orient=\"records\")\n",
    "\n",
    "        # Ask the user where to save the json file. This meets the client functionality\n",
    "        json_file_path = filedialog.asksaveasfilename(defaultextension=\".json\", filetypes=((\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")))\n",
    "\n",
    "        if json_file_path:\n",
    "            # Save as json\n",
    "            with open(json_file_path, 'w') as json_file:\n",
    "                json_file.write(json_str)\n",
    "\n",
    "            text_box.insert(tk.END, f\"JSON file saved successfully at {json_file_path}.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error saving file: {e}\\n\")\n",
    "\n",
    "\n",
    "    # Add buttons to dialog\n",
    "    csv_button = tk.Button(dialog, text=\"CSV\", command=lambda: save_as_filetype('csv'))\n",
    "    csv_button.pack(side=\"left\")\n",
    "    json_button = tk.Button(dialog, text=\"JSON\", command=lambda: save_as_filetype('json'))\n",
    "    json_button.pack(side=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c42845-4b0e-4468-9ec7-6312c70a8a98",
   "metadata": {},
   "source": [
    "<a id=\"Extract-DAB-Function\"></a>\n",
    "### Extract DAB Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc12f4e6-1281-415a-b053-a99e45395bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DAB function. \n",
    "# This sets up actions 2,3,4 by extracting the DAB multiplexes. \n",
    "# Allows the user to choose the DAB multiplexes. This  meets the functionality requirement to 'handle other sets of data'\n",
    "# This also creates the requested EID columns and fills them with values '1' or '0' to represent a match. \n",
    "\n",
    "import tkinter.simpledialog as simpledialog\n",
    "\n",
    "def extract_data():\n",
    "    global is_data_loaded\n",
    "    global is_data_cleaned\n",
    "    global is_data_extracted\n",
    "    if not is_data_loaded or not is_data_cleaned:\n",
    "        text_box.insert(tk.END, \"Please load and clean the data.\\n\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        global dataframes  # modifying the global DataFrame\n",
    "\n",
    "        # Asking user to specify multiplexes. This allows the functionality for other sets of data\n",
    "        user_input = simpledialog.askstring(\"Input\", \"Please enter DAB for extraction separated by a space. Suggested: C18A C18F C188.\")\n",
    "        \n",
    "        # Error check and feedback for closing selection box\n",
    "        if user_input is None:\n",
    "            text_box.insert(tk.END, \"Data extraction cancelled by user.\\n\")\n",
    "            return\n",
    "\n",
    "        # Convert user input string to a list of dataframe names\n",
    "        multiplexes = user_input.split()\n",
    "\n",
    "        # Validate multiplexes \n",
    "        valid_multiplexes = dataframes[0]['EID'].unique().tolist()  # get the unique EIDs from the DataFrame to check if they exist\n",
    "        for multiplex in multiplexes:\n",
    "            if multiplex not in valid_multiplexes:\n",
    "                text_box.insert(tk.END, f\"Invalid DAB entered: {multiplex}. Please enter valid DABs.\\n\")\n",
    "                return\n",
    "\n",
    "        # Filter df based on the user-specified multiplexes\n",
    "        dataframes[0] = dataframes[0][dataframes[0]['EID'].isin(multiplexes)]\n",
    "\n",
    "        # Add new columns for each selected multiplex in df\n",
    "        for multiplex in multiplexes:\n",
    "            dataframes[0][multiplex] = dataframes[0]['EID'].apply(lambda x: 1 if x == multiplex else 0)\n",
    "        \n",
    "        text_box.insert(tk.END, \"Extraction successful. DataFrame updated with selected EIDs.\\n\")\n",
    "        is_data_extracted = True\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error extracting data: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1bbfa8-fb66-4cac-8d39-5645cd885feb",
   "metadata": {},
   "source": [
    "<a id=\"Action-2-Join-JSON-Dictionary\"></a>\n",
    "### Action 2: Join JSON 'Dictionary'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113d9585-2bcd-4770-a7ba-899ff4400191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action (2) Join JSON 'dictionary'\n",
    "# This meets the specific column requirements to represent the client data as a 3-tier JSON dictionary with EID as the top tier then\n",
    "# NGR at the second tier and then beneath NGR is every other column. This can be viewed in the GUI tabs as well as saved locally on the disk\n",
    "\n",
    "def create_json_and_preview():\n",
    "    global is_data_loaded\n",
    "    global is_data_cleaned\n",
    "    global is_data_extracted\n",
    "    if not is_data_loaded or not is_data_cleaned or not is_data_extracted:\n",
    "        text_box.insert(tk.END, \"Please load, clean, and extract the data.\\n\")\n",
    "        return\n",
    "    try:\n",
    "        # Your existing JSON generation code...\n",
    "        df_json = dataframes[0].copy()\n",
    "        df_json['DATE'] = df_json['DATE'].apply(lambda x: x.isoformat() if not pd.isnull(x) else '')\n",
    "        nested_dict = df_json.groupby('EID').apply(lambda x: x.groupby('NGR')[['SITE', 'SITE HEIGHT', 'AERIAL HEIGHT(M)', 'POWER(KW)', 'DATE']].apply(lambda y: y.to_dict('records')).to_dict()).to_dict()\n",
    "        \n",
    "        json_str = json.dumps(nested_dict, indent=2)\n",
    "\n",
    "        # Ask the user where to save the json file\n",
    "        json_file_path = filedialog.asksaveasfilename(defaultextension=\".json\", filetypes=((\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")))\n",
    "\n",
    "        if json_file_path:\n",
    "            # Save as json\n",
    "            with open(json_file_path, 'w') as json_file:\n",
    "                json_file.write(json_str)\n",
    "\n",
    "            text_box.insert(tk.END, \"JSON file created successfully.\\n\")\n",
    "\n",
    "        # Create a new tab\n",
    "        tab = ttk.Frame(tab_parent)\n",
    "        tab_parent.add(tab, text = 'JSON Preview')\n",
    "\n",
    "        # Create a text widget in the new tab\n",
    "        text_widget = tk.Text(tab)\n",
    "        text_widget.insert(tk.END, json_str)\n",
    "        text_widget.pack()\n",
    "\n",
    "        text_box.insert(tk.END, \"JSON preview created successfully.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error creating JSON preview: {e}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d5b50-f7a0-4ca5-94f2-03a667d708b1",
   "metadata": {},
   "source": [
    "<a id=\"Action-3-Filter-and-Mean-Median-Mode\"></a>\n",
    "### Action 3: Mean, median, mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2054ca6c-b30c-4187-89ca-08e7f0625cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action 3: Mean, median, mode\n",
    "# This takes the requested DAB set and creates two statistical sets based on the requested filters (date & site height)\n",
    "# it uses pandas for the calculations \n",
    "\n",
    "def calculate_stats_and_preview():\n",
    "    global is_data_loaded\n",
    "    global is_data_cleaned\n",
    "    global is_data_extracted\n",
    "    if not is_data_loaded or not is_data_cleaned or not is_data_extracted:\n",
    "        text_box.insert(tk.END, \"Please load, clean, and extract the data.\\n\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        if len(dataframes) == 0:\n",
    "            text_box.insert(tk.END, \"Please load and clean the data before calculating stats.\\n\")\n",
    "            return\n",
    "\n",
    "        df = dataframes[0]\n",
    "\n",
    "        # Filter based on 'Site Height'\n",
    "        df_filtered_height = df[df['SITE HEIGHT'] > 75]\n",
    "\n",
    "        # Filter based on 'Date'\n",
    "        df_filtered_year = df[df['DATE'].dt.year >= 2001]\n",
    "\n",
    "        # Calculate stats for df_filtered_height\n",
    "        mean_power_height = df_filtered_height['POWER(KW)'].mean()\n",
    "        median_power_height = df_filtered_height['POWER(KW)'].median()\n",
    "        mode_power_height = df_filtered_height['POWER(KW)'].mode()[0] if not df_filtered_height['POWER(KW)'].mode().empty else \"No mode\"\n",
    "\n",
    "        # Calculate stats for df_filtered_year\n",
    "        mean_power_year = df_filtered_year['POWER(KW)'].mean()\n",
    "        median_power_year = df_filtered_year['POWER(KW)'].median()\n",
    "        mode_power_year = df_filtered_year['POWER(KW)'].mode()[0] if not df_filtered_year['POWER(KW)'].mode().empty else \"No mode\"\n",
    "\n",
    "        # Prepare the results string\n",
    "        stats_str = (\n",
    "            f\"Stats for 'Site Height' > 75:\\n\"\n",
    "            f\"Mean of 'Power(kW)': {mean_power_height}\\n\"\n",
    "            f\"Median of 'Power(kW)': {median_power_height}\\n\"\n",
    "            f\"Mode of 'Power(kW)': {mode_power_height}\\n\\n\"\n",
    "            f\"Stats for Year >= 2001:\\n\"\n",
    "            f\"Mean of 'Power(kW)': {mean_power_year}\\n\"\n",
    "            f\"Median of 'Power(kW)': {median_power_year}\\n\"\n",
    "            f\"Mode of 'Power(kW)': {mode_power_year}\\n\"\n",
    "        )\n",
    "\n",
    "        # Create a new tab\n",
    "        tab = ttk.Frame(tab_parent)\n",
    "        tab_parent.add(tab, text = 'Power(kW) Stats')\n",
    "\n",
    "        # Create a text widget in the new tab\n",
    "        text_widget = tk.Text(tab)\n",
    "        text_widget.insert(tk.END, stats_str)\n",
    "        text_widget.pack()\n",
    "\n",
    "        text_box.insert(tk.END, \"Stats calculated and previewed successfully.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error calculating and previewing stats: {e}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda97491-610a-4964-ab07-ecbd7039928a",
   "metadata": {},
   "source": [
    "<a id=\"Action-4-Correlation-Chi-square-Test-Function\"></a>\n",
    "### Action 4: Correlation: Chi-square Test Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c400021d-e905-430d-8de4-b031986882dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action (4) Correlation - Function to calculate Chi-square test\n",
    "# This takes the specific labels to check for correlation in a list and then uses the scipy chi-sqaure test to find the values for \n",
    "# c, p, dof, expected. It also filters the results based on the significance level of .05 to display two groups in the GUI. \n",
    "\n",
    "\n",
    "def calc_chi_square():\n",
    "    global is_data_loaded\n",
    "    global is_data_cleaned\n",
    "    global is_data_extracted\n",
    "    if not is_data_loaded or not is_data_cleaned or not is_data_extracted:\n",
    "        text_box.insert(tk.END, \"Please load, clean, and extract the data.\\n\")\n",
    "        return\n",
    "\n",
    "    significance_level = 0.05\n",
    "    df = dataframes[0].copy()\n",
    "    labels = ['SITE', 'FREQ', 'BLOCK', 'SERV LABEL1', 'SERV LABEL2', 'SERV LABEL3', 'SERV LABEL4', 'SERV LABEL10']\n",
    "    \n",
    "    significant_results = []\n",
    "    not_significant_results = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i+1, len(labels)):\n",
    "            crosstab = pd.crosstab(df[labels[i]], df[labels[j]])\n",
    "            # Chi-square test of independence.\n",
    "            c, p, dof, expected = chi2_contingency(crosstab)\n",
    "            \n",
    "            result_string = (f\"Between {labels[i]} and {labels[j]}:\\n\"\n",
    "                             f\"P-value: {p:.4f}\\n\"\n",
    "                             f\"Chi-square: {c:.4f}\\n\"\n",
    "                             f\"Degrees of Freedom: {dof}\\n\"\n",
    "                             f\"Expected Frequencies:\\n {expected}\\n\\n\")\n",
    "\n",
    "            if p < significance_level:\n",
    "                significant_results.append(result_string)\n",
    "            else:\n",
    "                not_significant_results.append(result_string)\n",
    "                \n",
    "    return {\"significant\": significant_results, \"not_significant\": not_significant_results}\n",
    "\n",
    "\n",
    "# This is the function to display the GUI for the correlation. Initially I only displayed the full detailed list but it was too long.\n",
    "# So I made two lists: an inital one with simple significant pairs and then the detailed results. \n",
    "def display_p_values(tab, p_values):\n",
    "    text_box = tk.Text(tab)\n",
    "    \n",
    "    # Displaying simple list of significant pairs\n",
    "    text_box.insert(tk.END, \"Significant Pairs:\\n\")\n",
    "    for value in p_values[\"significant\"]:\n",
    "        pair = value.split(\"\\n\")[0].replace(\"P-value between \", \"\").split(\" and \")\n",
    "        text_box.insert(tk.END, f\"{pair[0]} - {pair[1]}\\n\")\n",
    "    text_box.insert(tk.END, \"\\n\")\n",
    "    \n",
    "    # Displaying simple list of non-significant pairs\n",
    "    text_box.insert(tk.END, \"Not Significant Pairs:\\n\")\n",
    "    for value in p_values[\"not_significant\"]:\n",
    "        pair = value.split(\"\\n\")[0].replace(\"P-value between \", \"\").split(\" and \")\n",
    "        text_box.insert(tk.END, f\"{pair[0]} - {pair[1]}\\n\")\n",
    "    text_box.insert(tk.END, \"\\n\")\n",
    "\n",
    "    # Displaying detailed results\n",
    "    text_box.insert(tk.END, \"\\n\\nDetailed Results:\\n\")\n",
    "    text_box.insert(tk.END, \"-\" * 50 + \"\\n\")\n",
    "    text_box.insert(tk.END, \"Significant Results:\\n\")\n",
    "    text_box.insert(tk.END, \"\\n\".join(p_values[\"significant\"]))\n",
    "    text_box.insert(tk.END, \"-\" * 50 + \"\\n\")\n",
    "    text_box.insert(tk.END, \"Not Significant Results:\\n\")\n",
    "    text_box.insert(tk.END, \"\\n\".join(p_values[\"not_significant\"]))\n",
    "    \n",
    "    text_box.pack()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62eb82d-961a-4a50-a32b-38341359ea97",
   "metadata": {},
   "source": [
    "<a id=\"Action-5-Visualize-Data\"></a>\n",
    "### Action 5: Visualize Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac034dd-a71e-4e11-994d-7c035f7daa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action (5) Visualize data\n",
    "# This creates a Facetgrid to display the relationship between the labels and the three EIDs. It is then put into a scrollable canvas\n",
    "\n",
    "def plot_label(df, tab_parent):\n",
    "    global is_data_loaded\n",
    "    global is_data_cleaned\n",
    "    global is_data_extracted\n",
    "    if not is_data_loaded or not is_data_cleaned or not is_data_extracted:\n",
    "        text_box.insert(tk.END, \"Please load, clean, and extract the data.\\n\")\n",
    "        return\n",
    "\n",
    "    labels = ['FREQ', 'SITE', 'BLOCK', 'SERV LABEL1', 'SERV LABEL2', 'SERV LABEL3', 'SERV LABEL4', 'SERV LABEL10']\n",
    "    df_melted = df.melt(id_vars=['EID'], value_vars=labels)\n",
    "\n",
    "    g = sns.FacetGrid(df_melted, col='EID', row='variable', height=4, aspect=1)\n",
    "    g.map_dataframe(sns.countplot, x='value')\n",
    "    g.set_axis_labels(\"Values\", \"Count\")\n",
    "    g.set_titles(col_template=\"{col_name} EID\", row_template=\"{row_name}\")\n",
    "    for ax in g.axes.flat:\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_rotation(90)\n",
    "\n",
    "    facet_tab = ttk.Frame(tab_parent)\n",
    "    tab_parent.add(facet_tab, text=\"EID FacetGrid\")\n",
    "\n",
    "    # Scrollable canvas setup\n",
    "    scrollable_canvas = tk.Canvas(facet_tab)\n",
    "    scrollable_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "    \n",
    "    scrollbar = tk.Scrollbar(facet_tab, orient=\"vertical\", command=scrollable_canvas.yview)\n",
    "    scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "    scrollable_canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    canvas_frame = tk.Frame(scrollable_canvas)\n",
    "    scrollable_canvas.create_window((0, 0), window=canvas_frame, anchor=\"nw\")\n",
    "\n",
    "    canvas_facet = FigureCanvasTkAgg(g.fig, master=canvas_frame)\n",
    "    canvas_facet.draw()\n",
    "    canvas_facet.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    canvas_frame.update_idletasks()  # Ensure the frame's size is updated\n",
    "    scrollable_canvas.config(scrollregion=scrollable_canvas.bbox(\"all\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c86401-f111-4b9f-be4a-302f56ac6b9c",
   "metadata": {},
   "source": [
    "<a id=\"Button-Functions\"></a>\n",
    "### Button Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3921106-760e-42cb-a209-92b349344a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button functions\n",
    "\n",
    "def load_file_button_command():\n",
    "    try:\n",
    "        load_data()\n",
    "        text_box.insert(tk.END, \"Data loaded successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error loading data: {e}\\n\")\n",
    "\n",
    "def save_file_button_command():\n",
    "    try:\n",
    "        save_file()\n",
    "        text_box.insert(tk.END, \"Data saved successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error saving file: {e}\\n\")\n",
    "\n",
    "def clean_data_button_command():\n",
    "    try:\n",
    "        clean_data()\n",
    "        text_box.insert(tk.END, \"Data cleaned successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error cleaning data: {e}\\n\")\n",
    "\n",
    "def extract_data_button_command():\n",
    "    try:\n",
    "        extract_data()\n",
    "        text_box.insert(tk.END, \"Data extracted successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error extracting data: {e}\\n\")\n",
    "\n",
    "def calculate_stats_button_command():\n",
    "    try:\n",
    "        calculate_stats_and_preview()\n",
    "        text_box.insert(tk.END, \"Stats calculated and previewed successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error calculating and previewing stats: {e}\\n\")\n",
    "\n",
    "def create_json_button_command():\n",
    "    try:\n",
    "        create_json_and_preview()\n",
    "        text_box.insert(tk.END, \"JSON file created and previewed successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error creating and previewing JSON: {e}\\n\")\n",
    "\n",
    "def plot_clustered_bars_button_command():\n",
    "    plot_label(dataframes[0], tab_parent)\n",
    "\n",
    "\n",
    "def plot_correlation_button_command():\n",
    "    p_values = calc_chi_square()\n",
    "    # Create a new tab\n",
    "    tab = ttk.Frame(tab_parent)\n",
    "    tab_parent.add(tab, text='Chi-Square Results')\n",
    "    # Display p-values in the new tab\n",
    "    display_p_values(tab, p_values)\n",
    "    tab_parent.select(tab)  # Select (bring to front) the new tab\n",
    "    # Display a message in the dialog box indicating successful calculation\n",
    "    text_box.insert(tk.END, \"Correlation calculation complete. Results displayed in a new tab.\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f52a6a-44ff-4adb-ae3b-c344b884e832",
   "metadata": {},
   "source": [
    "<a id=\"Closing-the-App-Functions\"></a>\n",
    "### Closing the App Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71bb37bd-8cae-48e4-8886-bfc2429a2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the app functions\n",
    "# These functions meet the client requirements to back up the the format along with the save function. \n",
    "# It does this by using the Tkinter filedialog module and allows users to specify a filename and location for saving. Pandas is used\n",
    "# for the conversion to JSON\n",
    "\n",
    "def on_closing():\n",
    "    if messagebox.askokcancel(\"Quit\", \"Do you want to quit?\"):\n",
    "        create_backup()  # Backup current state\n",
    "        root.destroy()\n",
    "\n",
    "def create_backup():\n",
    "    global dataframes\n",
    "    if len(dataframes) == 0:\n",
    "        text_box.insert(tk.END, \"No data to backup.\\n\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Ask the user where to save the JSON file\n",
    "        json_file_path = filedialog.asksaveasfilename(defaultextension=\".json\", filetypes=((\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")))\n",
    "\n",
    "        if json_file_path:\n",
    "            # Create a copy of the global dataframe for JSON serialization\n",
    "            df_json = dataframes[0].copy()\n",
    "\n",
    "            # Convert 'DATE' column to string in ISO format in the copied dataframe\n",
    "            df_json['DATE'] = df_json['DATE'].apply(lambda x: x.isoformat() if not pd.isnull(x) else '')\n",
    "\n",
    "            # Save as JSON\n",
    "            with open(json_file_path, 'w') as json_file:\n",
    "                df_json.to_json(json_file, orient='records', lines=True)\n",
    "\n",
    "            text_box.insert(tk.END, f\"JSON backup created successfully at {json_file_path}.\\n\")\n",
    "        else:\n",
    "            text_box.insert(tk.END, \"Backup operation cancelled.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        text_box.insert(tk.END, f\"Error creating JSON backup: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48fa1a-d3ec-4c7e-81b9-db9e877b3f0a",
   "metadata": {},
   "source": [
    "<a id=\"GUI-and-Roots\"></a>\n",
    "### GUI and Roots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbfaee-e7de-4b29-89be-ba394091bbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 00:16:23.886 Python[35615:2903071] +[CATransaction synchronize] called within transaction\n",
      "2023-08-18 00:16:23.965 Python[35615:2903071] +[CATransaction synchronize] called within transaction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: /Users/jp/Downloads/TxAntennaDAB.csv...\n",
      "File loaded successfully with encoding ascii\n",
      "Loading file: /Users/jp/Downloads/TxParamsDAB.csv...\n",
      "File loaded successfully with encoding ISO-8859-1\n",
      "Files joined successfully on 'id' column\n",
      "Preview of the joined dataframe:\n",
      "        id         NGR  Longitude/Latitude  Site Height  In-Use Ae Ht\n",
      "0  745392  NO76418994  002W23 24 57N00 00          325           230\n",
      "1  745393  NJ76043299  002W24 01 57N23 12          245           138\n",
      "2  745394  NJ98315700  002W01 48 57N36 11          225            35\n",
      "3  745395  NJ94270253  002W05 46 57N06 49           87            41\n",
      "4  745396  NS29181617  004W41 59 55N24 35          273            34\n"
     ]
    }
   ],
   "source": [
    "#GUI and root\n",
    "\n",
    "\n",
    "# Initialize Tkinter root window\n",
    "root = tk.Tk()\n",
    "\n",
    "# Set the title of the window\n",
    "root.title(\"DAB Data Management\")\n",
    "\n",
    "# root.protocol goes after on_closing is defined\n",
    "root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "# Initialize Notebook (tab manager)\n",
    "tab_parent = ttk.Notebook(root)\n",
    "\n",
    "# Create and pack text box\n",
    "text_box = tk.Text(root)\n",
    "\n",
    "# ttk modern theme\n",
    "style = ttk.Style()\n",
    "style.theme_use(\"clam\")  # 'clam' is a modern-looking theme\n",
    "\n",
    "# setting font for all buttons\n",
    "button_font = (\"Arial\", 24, \"bold\")\n",
    "\n",
    "# setting the color for each button\n",
    "colors = [\"#B200ED\", \"#B200ED\", \"#B200ED\", \"#0B6623\", \"#50C878\", \"#50C878\", \"#50C878\", \"#50C878\", \"#50C878\"]\n",
    "\n",
    "# Create buttons with their corresponding command functions\n",
    "buttons = [\n",
    "    (\"LOAD DATA\", load_file_button_command),\n",
    "    (\"CLEAN DATA\", clean_data_button_command),\n",
    "    (\"SAVE DATA AS .json\", save_file_button_command),\n",
    "    (\"EXTRACT DAB: C18A, C18F, C188\", extract_data),\n",
    "    (\"(2)JOIN JSON DICT\", create_json_button_command),\n",
    "    (\"(3)CALC MEAN/MEDIAN/MODE\", calculate_stats_and_preview),\n",
    "    (\"(4)VISUALIZE\", plot_clustered_bars_button_command),\n",
    "    (\"(5)CORRELATIONS\", plot_correlation_button_command)\n",
    "]\n",
    "\n",
    "# Update the button style for all buttons\n",
    "for i, (text, command) in enumerate(buttons):\n",
    "    button_style = f'Accentbutton{i}.TButton'  \n",
    "    style.configure(button_style, foreground='white', background=colors[i], font=button_font)\n",
    "    button = ttk.Button(root, text=text, command=command, style=button_style)\n",
    "    button.grid(row=i, column=0, padx=5, pady=5, sticky='ew')  \n",
    "\n",
    "# Arrange text box and tabs using grid\n",
    "text_box.grid(row=10, column=0, padx=5, pady=5, sticky='nswe')  # padx and pady add some padding around the text box\n",
    "tab_parent.grid(row=0, column=1, rowspan=11, padx=5, pady=5, sticky='nswe')  # rowspan=11 makes the notebook span 11 rows\n",
    "\n",
    "# Configure rows and columns for proper resizing\n",
    "root.grid_rowconfigure(10, weight=1)  # The text box is now in row 10\n",
    "root.grid_columnconfigure(1, weight=1)\n",
    "\n",
    "# Enter the Tkinter main event loop\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef85341-df7c-4e52-b74c-8986ab9c207a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1ab21-b4a0-4ea6-a413-c7a41ead2ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
